{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised models for feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the literature, the supervised models used for trajectory classification are usually SVM and KNN. In this notebook we are going to test the effectiveness of these models, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we are going to load the vectors where the trajectories are described by their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\r"
     ]
    }
   ],
   "source": [
    "import feature_vec as fv\n",
    "\n",
    "metadata = fv.get_selected_data()\n",
    "feat_vectors, clss_mask, clss = fv.get_feat_vectors(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the data into 70% for model training and the other 30% for validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat_vectors, clss, stratify=clss, \n",
    "                                                  random_state = 0, test_size=0.30)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, stratify=y_test, \n",
    "                                                  random_state = 0, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whith number of neighbors by default for kneighbors queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7054726368159204"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_val, y_val)\n",
    "\n",
    "# 0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results with the weights parameter with value `distance` are better than `uniform` according to experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whith number of neighbors on 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7223880597014926"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(weights='distance', n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_val, y_val)\n",
    "\n",
    "# 0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see, this model does not achieve much more precision. Let's try the other recommended model, the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with SVM, we will standardize the values ​​of the features and with pipelines we will give these new values ​​to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a polynomial kernel of degree 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7114427860696517"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = make_pipeline(StandardScaler(), SVC(kernel='poly', degree=3, gamma='scale', random_state = 0))\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_val, y_val)\n",
    "\n",
    "# 0.71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with a sigmoid kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7522388059701492"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = make_pipeline(StandardScaler(), SVC(kernel='sigmoid', gamma='auto', random_state = 0))\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_val, y_val)\n",
    "\n",
    "# 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got better :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try it with an rbf kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83681592039801"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "svm = make_pipeline(StandardScaler(), SVC(kernel = 'rbf', gamma='auto', probability=True, random_state = 0))\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_val, y_val)\n",
    "\n",
    "# 0.83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8371\n",
      "AUC: 0.9579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred=svm.predict(X_test))\n",
    "auc_score = roc_auc_score(y_test, svm.predict_proba(X_test)[:], multi_class='ovr')\n",
    "print(f\"Accuracy: {acc_score:0.4f}\")\n",
    "print(f\"AUC: {auc_score:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the best we have achieved so far. This is not a bad result, but we will try other classic supervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a basic decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8308457711442786"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(criterion='entropy', random_state = 0)\n",
    "dtc.fit(X_train, y_train)\n",
    "dtc.score(X_val, y_val)\n",
    "\n",
    "# 0.83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, this model looks good, what if we put steroids on it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.900497512437811"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(criterion='entropy', max_features='log2', bootstrap=False, random_state=0)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_val, y_val)\n",
    "\n",
    "# 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iqr_velocity', 0.11873743232587498)\n",
      "('mean_velocity', 0.1094967549799762)\n",
      "('stop_rate', 0.08403357148109494)\n",
      "('std_velocity', 0.06461460640924746)\n",
      "('velocity_change_rate', 0.0642636588838411)\n",
      "('iqr_turning_angle', 0.05501240686834602)\n",
      "('iqr_heading_change_rate', 0.045699769843295976)\n",
      "('distance', 0.043231525433121094)\n",
      "('median_velocity', 0.03803674932408078)\n",
      "('coef_var_velocity', 0.027088260485747584)\n",
      "('max_velocity', 0.026846961766626053)\n",
      "('min_angle', 0.01511989004105316)\n",
      "('std_turning_angle', 0.014554040032265155)\n",
      "('iqr_acc_change_rate', 0.014480041253424156)\n",
      "('iqr_angle', 0.013657525779463916)\n",
      "('max_turning_angle', 0.012944970402356369)\n",
      "('var_turning_angle', 0.012377904291021914)\n",
      "('std_heading_change_rate', 0.01227608026371222)\n",
      "('min_acceleration', 0.012192524419444005)\n",
      "('var_angle', 0.01215148449788078)\n",
      "('max_acc_change_rate', 0.012071222539080226)\n",
      "('std_acc_change_rate', 0.011838734025787962)\n",
      "('min_acc_change_rate', 0.011285571139051938)\n",
      "('max_acceleration', 0.010603148976072707)\n",
      "('std_angle', 0.010269868073474324)\n",
      "('max_heading_change_rate', 0.010053319301361755)\n",
      "('std_acceleration', 0.009833572637014568)\n",
      "('min_heading_change_rate', 0.009627406052828866)\n",
      "('var_heading_change_rate', 0.008950322236187244)\n",
      "('iqr_acceleration', 0.008792682379454348)\n",
      "('mean_angle', 0.008684161433337115)\n",
      "('min_turning_angle', 0.00852070207979682)\n",
      "('coef_var_angle', 0.008100189092743022)\n",
      "('coef_var_acc_change_rate', 0.00722697269425363)\n",
      "('coef_var_heading_change_rate', 0.006675256644875043)\n",
      "('coef_var_acceleration', 0.006532264303358036)\n",
      "('median_angle', 0.006516195825761888)\n",
      "('coef_var_turning_angle', 0.0064845642030440815)\n",
      "('mean_heading_change_rate', 0.006119127522402322)\n",
      "('mean_turning_angle', 0.005816346376309127)\n",
      "('median_heading_change_rate', 0.005590642087932197)\n",
      "('median_turning_angle', 0.005121070045159032)\n",
      "('median_acc_change_rate', 0.004873283343713795)\n",
      "('median_acceleration', 0.004771807094410466)\n",
      "('min_velocity', 0.0036702981546554323)\n",
      "('max_angle', 0.0024248800896798403)\n",
      "('mean_acceleration', 0.0016821150682511134)\n",
      "('mean_acc_change_rate', 0.0010190155813889087)\n",
      "('var_acc_change_rate', 2.446057922842131e-05)\n",
      "('var_velocity', 4.641637512088754e-06)\n",
      "('var_acceleration', 0.0)\n"
     ]
    }
   ],
   "source": [
    "importances = list(zip(fv.feat_name, rfc.feature_importances_))\n",
    "print(*sorted(importances, key=lambda x: -x[1]), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great, let's explore some combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8049751243781095"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1 = Pipeline([('pca', PCA(n_components = 15)), \n",
    "                     ('Random_Forest', \n",
    "                      RandomForestClassifier(criterion='entropy', max_features='log2', bootstrap=False, random_state=0))])\n",
    "\n",
    "pipe1.fit(X_train, y_train)\n",
    "pipe1.score(X_val, y_val)\n",
    "\n",
    "# 0.80 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955223880597015"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2 = Pipeline([('ste', StandardScaler()),\n",
    "                     ('Random_Forest', \n",
    "                      RandomForestClassifier(criterion='entropy', max_features='log2', bootstrap=False, random_state=0))])\n",
    "\n",
    "pipe2.fit(X_train, y_train)\n",
    "pipe2.score(X_val, y_val)\n",
    "\n",
    "# 0.89 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for a good combination of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search based on out-of-bag score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 5, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 5, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 5, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 9, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 9, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 9, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 12, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 12, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 12, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 20, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 20, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': None, 'max_features': 20, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 5, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 5, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 5, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 7, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 7, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 7, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 9, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 9, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 9, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 12, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 12, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 12, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 20, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 20, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 3, 'max_features': 20, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 5, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 5, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 5, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 7, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 7, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 7, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 9, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 9, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 9, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 12, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 12, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 12, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 20, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 20, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 10, 'max_features': 20, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 5, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 5, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 5, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 7, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 7, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 7, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 9, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 9, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 9, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 12, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 12, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 12, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 20, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 20, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'gini', 'max_depth': 20, 'max_features': 20, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 5, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 5, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 5, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 7, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 7, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 7, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 12, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 12, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 12, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 20, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 20, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': None, 'max_features': 20, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 5, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 5, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 5, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 7, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 7, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 7, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 9, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 9, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 9, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 12, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 12, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 12, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 20, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 20, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 20, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 5, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 5, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 5, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 7, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 7, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 7, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 9, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 9, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 9, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 12, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 12, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 12, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 20, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 20, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 20, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 5, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 5, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 5, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 7, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 7, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 7, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 9, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 9, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 9, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 12, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 12, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 12, 'n_estimators': 200} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 20, 'n_estimators': 100} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 20, 'n_estimators': 150} ✓\n",
      "Model: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 20, 'n_estimators': 200} ✓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oob_accuracy</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.892029</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.892029</td>\n",
       "      <td>entropy</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.891304</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.891304</td>\n",
       "      <td>entropy</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.891304</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     oob_accuracy criterion  max_depth  max_features  n_estimators\n",
       "62       0.892029   entropy        NaN             5           200\n",
       "107      0.892029   entropy       20.0             5           200\n",
       "104      0.891304   entropy       10.0            20           200\n",
       "112      0.891304   entropy       20.0             9           150\n",
       "67       0.891304   entropy        NaN             9           150"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "param_grid = ParameterGrid(\n",
    "                {'n_estimators': [100, 150, 200],\n",
    "                 'max_features': [5, 7, 9, 12, 20],\n",
    "                 'max_depth'   : [None, 3, 10, 20],\n",
    "                 'criterion'   : ['gini', 'entropy']\n",
    "                }\n",
    "            )\n",
    "\n",
    "results = {'params': [], 'oob_accuracy': []}\n",
    "\n",
    "for params in param_grid:\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 0,\n",
    "                ** params\n",
    "             )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    results['params'].append(params)\n",
    "    results['oob_accuracy'].append(model.oob_score_)\n",
    "    print(f\"Model: {params} \\u2713\")\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results = pd.concat([results, results['params'].apply(pd.Series)], axis=1)\n",
    "results = results.sort_values('oob_accuracy', ascending=False)\n",
    "results = results.drop(columns = 'params')\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search based on cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "import multiprocessing\n",
    "\n",
    "param_grid = {'n_estimators': [150, 200],\n",
    "            'max_features': [5, 7, 9, 15, 25],\n",
    "            'max_depth'   : [None, 3, 10, 20, 30],\n",
    "            'criterion'   : ['gini', 'entropy']\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(\n",
    "        estimator  = RandomForestClassifier(random_state = 0),\n",
    "        param_grid = param_grid,\n",
    "        scoring    = 'accuracy',\n",
    "        n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "        cv         = RepeatedKFold(n_splits=5, n_repeats=3, random_state=0), \n",
    "        refit      = True,\n",
    "        verbose    = 0,\n",
    "        return_train_score = True\n",
    "       )\n",
    "\n",
    "grid.fit(X = X_train, y = y_train)\n",
    "\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False) \\\n",
    "    .head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "87/87 [==============================] - 1s 2ms/step - loss: 1.5040 - accuracy: 0.2993\n",
      "Epoch 2/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4895 - accuracy: 0.2924\n",
      "Epoch 3/30\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 1.4875 - accuracy: 0.3051\n",
      "Epoch 4/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4850 - accuracy: 0.3130\n",
      "Epoch 5/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4897 - accuracy: 0.3040\n",
      "Epoch 6/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4896 - accuracy: 0.3036\n",
      "Epoch 7/30\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 1.4871 - accuracy: 0.3072\n",
      "Epoch 8/30\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 1.4872 - accuracy: 0.2996\n",
      "Epoch 9/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4871 - accuracy: 0.3083\n",
      "Epoch 10/30\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 1.4856 - accuracy: 0.3221\n",
      "Epoch 11/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4831 - accuracy: 0.3170\n",
      "Epoch 12/30\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 1.4843 - accuracy: 0.3134\n",
      "Epoch 13/30\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 1.4834 - accuracy: 0.3178\n",
      "Epoch 14/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4831 - accuracy: 0.3286\n",
      "Epoch 15/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4846 - accuracy: 0.3054\n",
      "Epoch 16/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4814 - accuracy: 0.3138\n",
      "Epoch 17/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4822 - accuracy: 0.3134\n",
      "Epoch 18/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4831 - accuracy: 0.3178\n",
      "Epoch 19/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4849 - accuracy: 0.3072\n",
      "Epoch 20/30\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 1.4841 - accuracy: 0.3116\n",
      "Epoch 21/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4843 - accuracy: 0.3087\n",
      "Epoch 22/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4845 - accuracy: 0.3159\n",
      "Epoch 23/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4822 - accuracy: 0.3268\n",
      "Epoch 24/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4847 - accuracy: 0.3127\n",
      "Epoch 25/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4841 - accuracy: 0.3120\n",
      "Epoch 26/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4816 - accuracy: 0.3272\n",
      "Epoch 27/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4808 - accuracy: 0.3250\n",
      "Epoch 28/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4788 - accuracy: 0.3333\n",
      "Epoch 29/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4795 - accuracy: 0.3293\n",
      "Epoch 30/30\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 1.4751 - accuracy: 0.3301\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(51, activation = 'sigmoid'))\n",
    "model.add(keras.layers.Dense(300, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(5, activation= 'softmax'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(np.array(X_train), np.array(y_train), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manoly/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/manoly/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/manoly/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/manoly/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01,\n",
       "              max_iter=5000, random_state=123, solver='lbfgs')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import multiprocessing\n",
    "\n",
    "model1 = MLPClassifier(\n",
    "                hidden_layer_sizes=(5),\n",
    "                learning_rate_init=0.01,\n",
    "                solver = 'lbfgs',\n",
    "                max_iter = 1000,\n",
    "                random_state = 123\n",
    "            )\n",
    "\n",
    "model2 = MLPClassifier(\n",
    "                hidden_layer_sizes=(10),\n",
    "                learning_rate_init=0.01,\n",
    "                solver = 'lbfgs',\n",
    "                max_iter = 1000,\n",
    "                random_state = 123\n",
    "            )\n",
    "\n",
    "model3 = MLPClassifier(\n",
    "                hidden_layer_sizes=(20, 20),\n",
    "                learning_rate_init=0.01,\n",
    "                solver = 'lbfgs',\n",
    "                max_iter = 5000,\n",
    "                random_state = 123\n",
    "            )\n",
    "\n",
    "model4 = MLPClassifier(\n",
    "                hidden_layer_sizes=(50, 50, 50),\n",
    "                learning_rate_init=0.01,\n",
    "                solver = 'lbfgs',\n",
    "                max_iter = 5000,\n",
    "                random_state = 123\n",
    "            )\n",
    "\n",
    "model1.fit(X=X_train, y=y_train)\n",
    "model2.fit(X=X_train, y=y_train)\n",
    "model3.fit(X=X_train, y=y_train)\n",
    "model4.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29850746268656714 0.21990049751243781 0.2407960199004975 0.07761194029850746\n"
     ]
    }
   ],
   "source": [
    "print(model1.score(X_val, y_val),\n",
    "      model2.score(X_val, y_val),\n",
    "      model3.score(X_val, y_val),\n",
    "      model4.score(X_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
